{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/kssocha/Desktop/Nauka/portfolio/2024-gold-price-prediction-with-lstm-model/data/raw/2024-04-06_09:28_yf_au_data_.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 4))\n",
    "plt.plot(df['Close_gold'])\n",
    "plt.xticks(range(0, df.shape[0], 500), df['Date'].loc[::500], rotation=45)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the percentage change\n",
    "df['returns'] = df['Close_gold'].pct_change()\n",
    "\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.plot(df['returns'])\n",
    "plt.xticks(range(0, df.shape[0], 500), df['Date'].loc[::500], rotation=45)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the log returns\n",
    "#https://quantivity.wordpress.com/2011/02/21/why-log-returns/\n",
    "df['log_returns'] = np.log(1 + df['returns'])\n",
    "\n",
    "plt.figure(1, figsize=(16,4))\n",
    "plt.plot(df['log_returns'])\n",
    "plt.xticks(range(0, df.shape[0], 500), df['Date'].loc[::500], rotation=45)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "#drop rows with missing values\n",
    "df = df.dropna(how='any')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create feature matrix X\n",
    "X = df[['Close_gold', 'log_returns']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data normalization with MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1)).fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "#create target vector y\n",
    "y = [x[0] for x in X_scaled]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data split into train, test and validation sets\n",
    "split_1 = int(len(X_scaled) * 0.8)\n",
    "split_2 = int(len(X_scaled) * 0.95)\n",
    "\n",
    "X_train = X_scaled[:split_1]\n",
    "X_validation = X_scaled[split_1 : split_2]\n",
    "X_test = X_scaled[split_2 : len(X_scaled)]\n",
    "y_train = y[:split_1]\n",
    "y_validation = y[split_1 : split_2]\n",
    "y_test = y[split_2 : len(y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the lengths\n",
    "assert len(X_train) == len(y_train)\n",
    "assert len(X_validation) == len(y_validation)\n",
    "assert len(X_test) == len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labeling\n",
    "n = 3\n",
    "Xtrain = []\n",
    "ytrain = []\n",
    "Xvalidation = []\n",
    "yvalidation = []\n",
    "Xtest = []\n",
    "ytest = []\n",
    "for i in range(n, len(X_train)):\n",
    "    Xtrain.append(X_train[i - n: i, : X_train.shape[1]])\n",
    "    ytrain.append(y_train[i])\n",
    "for i in range(n, len(X_test)):\n",
    "    Xtest.append(X_test[i - n: i, : X_test.shape[1]])\n",
    "    ytest.append(y_test[i])\n",
    "for i in range(n,len(X_validation)):\n",
    "    Xvalidation.append(X_validation[i - n: i, : X_validation.shape[1]])\n",
    "    yvalidation.append(y_validation[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#revers transformation example\n",
    "val = np.array(ytrain[0])\n",
    "val = np.c_[val, np.zeros(val.shape)]\n",
    "result = scaler.inverse_transform(val)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM inputs reshaping\n",
    "Xtrain, ytrain = (np.array(Xtrain), np.array(ytrain))\n",
    "Xtrain = np.reshape(Xtrain, (Xtrain.shape[0], Xtrain.shape[1], Xtrain.shape[2]))\n",
    "\n",
    "Xvalidation, yvalidation = (np.array(Xvalidation), np.array(yvalidation))\n",
    "Xvalidation = np.reshape(Xvalidation, (Xvalidation.shape[0], Xvalidation.shape[1], Xvalidation.shape[2]))\n",
    "\n",
    "Xtest, ytest = (np.array(Xtest), np.array(ytest))\n",
    "Xtest = np.reshape(Xtest, (Xtest.shape[0], Xtest.shape[1], Xtest.shape[2]))\n",
    "\n",
    "print(Xtrain.shape)\n",
    "print(ytrain.shape)\n",
    "print(Xvalidation.shape)\n",
    "print(yvalidation.shape)\n",
    "print(Xtest.shape)\n",
    "print(ytest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers, models, metrics, activations\n",
    "from keras.metrics import mean_squared_error, mean_absolute_error\n",
    "from keras.activations import tanh, relu, sigmoid\n",
    "from keras.layers import Dense, LSTM, Dropout, Input\n",
    "from keras.models import Sequential, save_model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.callbacks import History, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_3l_model(lstm_nodes_1 = 20, lstm_nodes_2 = 30, lstm_nodes_3 = 30, dense_nodes_1 = 20,\n",
    "                   input_shape = (Xtrain.shape[1], Xtrain.shape[2]),\n",
    "                   dropout = 0.05, activation_lstm = 'tanh', activation_dense = 'tanh', loss = 'mean_squared_error', learning_rate = 0.00005):\n",
    "    model = Sequential([\n",
    "        Input(shape = input_shape),\n",
    "        LSTM(lstm_nodes_1, dropout = dropout, activation = activation_lstm, return_sequences = True),\n",
    "        LSTM(lstm_nodes_2, dropout = dropout, activation = activation_lstm, return_sequences = True),\n",
    "        LSTM(lstm_nodes_3, dropout = dropout, activation = activation_lstm),\n",
    "        Dense(dense_nodes_1, activation = activation_dense),\n",
    "        Dense(1)\n",
    "        ])\n",
    "    \n",
    "    model.compile(loss = loss,\n",
    "                  optimizer = Adam(learning_rate = learning_rate),\n",
    "                  metrics = [mean_squared_error, mean_absolute_error])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$\n",
    "\n",
    "$n$ is the number of samples in the dataset\n",
    "\n",
    "$y_i$ is the actual target value for the ith sample\n",
    "\n",
    "$\\hat{y}_i$ is the predicted value for the ith sample\n",
    "\n",
    "\n",
    "$MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|$\n",
    "\n",
    "$n$ is the number of samples in the dataset\n",
    "\n",
    "$y_i$ is the actual target value for the ith sample\n",
    "\n",
    "$\\hat{y}_i$ is the predicted value for the ith sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations_summary = pd.DataFrame(columns = ['iteration', 'loss', 'val_loss', 'mean_squared_error','val_mean_squared_error',\n",
    "                                             'mean_absolute_error', 'val_mean_absolute_error', 'stopped_epoch', 'best_performer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Monte Carlo dropout training set up\n",
    "batch_size = 64\n",
    "epochs = 5_000\n",
    "patience = 500\n",
    "start_from_epoch = 1_500\n",
    "#validation data set\n",
    "validation_data = (Xvalidation, yvalidation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Monte Carlo dropout training\n",
    "from datetime import datetime\n",
    "import  json\n",
    "import os\n",
    "\n",
    "iterations_summary = iterations_summary[0:0]\n",
    "time_stamp = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss', patience = patience, verbose = 1, restore_best_weights = True,\n",
    "                               start_from_epoch = start_from_epoch)\n",
    "\n",
    "iterations_summary = pd.DataFrame(columns = ['iteration', 'loss', 'val_loss', 'mean_squared_error','val_mean_squared_error',\n",
    "                                             'mean_absolute_error', 'val_mean_absolute_error', 'stopped_epoch'])\n",
    "\n",
    "def monte_carlo_dropout(Xtrain = Xtrain, ytrain = ytrain, validation_data = validation_data,\n",
    "                        epochs = epochs, batch_size = batch_size, early_stopping = early_stopping,\n",
    "                        n = 100):\n",
    "    for iteration in range(n):\n",
    "        \n",
    "        model = build_3l_model(lstm_nodes_1 = 20, lstm_nodes_2 = 30, lstm_nodes_3 = 30, dense_nodes_1 = 20,\n",
    "                       input_shape = (Xtrain.shape[1], Xtrain.shape[2]), dropout = 0.05, activation_lstm = 'tanh',\n",
    "                       activation_dense = 'tanh', loss = 'mean_squared_error', learning_rate = 0.00005)\n",
    "        \n",
    "        fitt_model = model.fit(Xtrain, ytrain, batch_size = batch_size, epochs = epochs, validation_data = validation_data,\n",
    "                               verbose = 0, callbacks = [early_stopping])\n",
    "        \n",
    "        stopped_epoch = early_stopping.stopped_epoch\n",
    "        \n",
    "        #save model\n",
    "        models_folder_path = '/home/kssocha/Desktop/Nauka/portfolio/2024-gold-price-prediction-with-lstm-model/models/{0}-monte-carlo/{0}-models'.format(time_stamp)\n",
    "        os.makedirs(models_folder_path, exist_ok = True)\n",
    "        model_files_path = '/home/kssocha/Desktop/Nauka/portfolio/2024-gold-price-prediction-with-lstm-model/models/{0}-monte-carlo/{0}-models/{0}_lstm_model_monte_carlo_{1}.keras'.format(time_stamp, iteration)\n",
    "        model.save(model_files_path)\n",
    "                         \n",
    "        #save model history\n",
    "        model_history = fitt_model.history\n",
    "        history_folder_path = '/home/kssocha/Desktop/Nauka/portfolio/2024-gold-price-prediction-with-lstm-model/models/{0}-monte-carlo/{0}-history'.format(time_stamp)\n",
    "        os.makedirs(history_folder_path, exist_ok = True)\n",
    "                                \n",
    "        history_files_path = '/home/kssocha/Desktop/Nauka/portfolio/2024-gold-price-prediction-with-lstm-model/models/{0}-monte-carlo/{0}-history/{0}_lstm_model_history_{1}.json'.format(time_stamp, iteration)\n",
    "        with open(history_files_path, 'w') as json_file:\n",
    "            json.dump(model_history, json_file)\n",
    "                                                                \n",
    "        iteration_row = {'iteration': iteration,\n",
    "                      'loss': fitt_model.history['loss'][-1],\n",
    "                      'val_loss': fitt_model.history['val_loss'][-1],\n",
    "                      'mean_squared_error': fitt_model.history['mean_squared_error'][-1],\n",
    "                      'val_mean_squared_error': fitt_model.history['val_mean_squared_error'][-1],\n",
    "                      'mean_absolute_error': fitt_model.history['mean_absolute_error'][-1],\n",
    "                      'val_mean_absolute_error': fitt_model.history['val_mean_absolute_error'][-1],\n",
    "                      'stopped_epoch': stopped_epoch}\n",
    "                                   \n",
    "        iterations_summary.loc[iteration] = iteration_row\n",
    "    \n",
    "    #save training summary to csv\n",
    "    return iterations_summary.to_csv('/home/kssocha/Desktop/Nauka/portfolio/2024-gold-price-prediction-with-lstm-model/models/{0}-monte-carlo/{0}_lstm_iterations_summary.csv'.format(time_stamp), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monte_carlo_dropout(Xtrain = Xtrain, ytrain = ytrain, validation_data = validation_data,\n",
    "                    epochs = epochs, batch_size = batch_size, early_stopping = early_stopping,\n",
    "                    n = 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
